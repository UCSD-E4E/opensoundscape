{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with pre-trained CNNs\n",
    "This notebook contains all the code you need to use a pre-trained OpenSoundscape convolutional neural network model (CNN) to make predictions on your own data. Before attempting this tutorial, install OpenSoundscape by following the instructions on the OpenSoundscape website, [opensoundscape.org](http://opensoundscape.org/). More detailed tutorials about data preprocessing, training CNNs, and customizing prediction methods can also be found on this site.\n",
    "\n",
    "Note that prediction no longer requires you to split your files into clips ahead of time - you can simply create a list of audio files of arbitrary length. Prediction scores will be generated on windows of a fixed length, eg 5 seconds, for the duration of each audio file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cnn` module provides a function `load_model` to load saved opensoundscape models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import load_model, load_outdated_model\n",
    "import opensoundscape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load some additional packages and perform some setup for the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other utilities and packages\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create and save a model object to use for demonstration in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 3 classes\n"
     ]
    }
   ],
   "source": [
    "from opensoundscape.torch.models.cnn import PytorchModel\n",
    "PytorchModel('resnet18',[0,1,2]).save('./temp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a saved model\n",
    "For this example, let's download a pre-trained model from the Kitzes Lab box to use as an example. This 2-class model is not actually good at recognizing any particular species, but it's useful for illustrating how prediction works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100     8    0     8    0     0      5      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 85.4M  100 85.4M    0     0  6533k      0  0:00:13  0:00:13 --:--:-- 10.5M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['curl', 'https://pitt.box.com/shared/static/s9lydizgspwsimo4p5l5j4nf9yeg319k.model', '-L', '-o', 'example.model'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['curl',\n",
    "               'https://pitt.box.com/shared/static/s9lydizgspwsimo4p5l5j4nf9yeg319k.model', \n",
    "                '-L', '-o', 'example.model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the model object using the `load_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./example.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose audio files for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of audio files to predict on. They can be of any length. Consider using `glob` to find many files at once.\n",
    "\n",
    "For this example, let's download a 1-minute audio clip from the Kitzes Lab box to use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100     7    0     7    0     0      5      0 --:--:--  0:00:01 --:--:--     5\n",
      "100 3750k  100 3750k    0     0  1150k      0  0:00:03  0:00:03 --:--:-- 1947k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['curl', 'https://pitt.box.com/shared/static/z73eked7quh1t2pp93axzrrpq6wwydx0.wav', '-L', '-o', '1min_audio.wav'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['curl',\n",
    "               'https://pitt.box.com/shared/static/z73eked7quh1t2pp93axzrrpq6wwydx0.wav', \n",
    "                '-L', '-o', '1min_audio.wav'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./1min_audio.wav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob \n",
    "audio_files = glob('./*.wav') #match all .wav files in the current directory\n",
    "audio_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a dataframe and dataset for prediction\n",
    "The prediction dataframe will have the names of each file and the start and end time of each window that we want to generate predictions for. OpenSoundscape provides a helper function to create this dataframe in the case where we want to predict on fixed-length windows with a fixed overlap between consecutive windows. Note that we need to know the duration of clips that the model expects, eg 5 second clips for the model we downloaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./1min_audio.wav</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./1min_audio.wav</th>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./1min_audio.wav</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./1min_audio.wav</th>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./1min_audio.wav</th>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  start_time  end_time\n",
       "file                                  \n",
       "./1min_audio.wav         0.0       5.0\n",
       "./1min_audio.wav         5.0      10.0\n",
       "./1min_audio.wav        10.0      15.0\n",
       "./1min_audio.wav        15.0      20.0\n",
       "./1min_audio.wav        20.0      25.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opensoundscape.helpers import make_clip_df\n",
    "clip_df = make_clip_df(files=audio_files,clip_duration=5.0)\n",
    "clip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we might need to change the preprocessing parameters of our Preprocessor object to match the model's preprocessing used during training (e.g. spectrogram parameters or bandpassing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.preprocess.preprocessors import ClipLoadingSpectrogramPreprocessor\n",
    "prediction_dataset = ClipLoadingSpectrogramPreprocessor(clip_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check on the parameters used during trainig by accessing model.train_dataset. For instance, check the bandpassing behavior of the training dataset:\n",
    "\n",
    "(Note that an empty params dictionary indicates that default values were used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_f': 0, 'max_f': 11025, 'out_of_bounds_ok': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_dataset.actions.bandpass.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_f': 0, 'max_f': 11025, 'out_of_bounds_ok': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_dataset.actions.bandpass.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate predictions with the model\n",
    "The model returns a dataframe with a MultiIndex of file, start_time, and end_time. There is one column for each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>absent</th>\n",
       "      <th>present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">./1min_audio.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>5.0</th>\n",
       "      <td>0.277279</td>\n",
       "      <td>-0.293570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <th>10.0</th>\n",
       "      <td>0.359079</td>\n",
       "      <td>-0.363364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>-1.124165</td>\n",
       "      <td>1.038807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <th>20.0</th>\n",
       "      <td>0.350858</td>\n",
       "      <td>-0.332194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <th>25.0</th>\n",
       "      <td>-3.864331</td>\n",
       "      <td>3.613129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        absent   present\n",
       "file             start_time end_time                    \n",
       "./1min_audio.wav 0.0        5.0       0.277279 -0.293570\n",
       "                 5.0        10.0      0.359079 -0.363364\n",
       "                 10.0       15.0     -1.124165  1.038807\n",
       "                 15.0       20.0      0.350858 -0.332194\n",
       "                 20.0       25.0     -3.864331  3.613129"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, _, _ = model.predict(prediction_dataset)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using models from older OpenSoundscape versions\n",
    "Models trained and saved with OpenSoundscape versions prior to 0.6.1 need to be loaded in a different way, and require that you know the architecture of the saved model. \n",
    "\n",
    "For example, one set of our publicly availably [binary models for 500 species](https://pitt.app.box.com/s/3048856qbm9x55yi3zfksa3fide5uuf4) was created with an older version of OpenSoundscape. These models require a little bit of manipulation to load into OpenSoundscape 0.5.x and onward. From the model notes page, we know that these models were trained with a resnet18 architecture. We can load them into a PytorchModel class. \n",
    "\n",
    "First, let's download one of these models (it's stored in a .tar format) and save it to the same directory as this notebook in a file called `opso_04_model_acanthis-flammea.tar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100     8    0     8    0     0      5      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 42.9M  100 42.9M    0     0  4704k      0  0:00:09  0:00:09 --:--:-- 8323k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['curl', 'https://pitt.box.com/shared/static/lglpty35omjhmq6cdz8cfudm43nn2t9f.tar', '-L', '-o', 'opso_04_model_acanthis-flammea.tar'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['curl',\n",
    "               'https://pitt.box.com/shared/static/lglpty35omjhmq6cdz8cfudm43nn2t9f.tar', \n",
    "                '-L', '-o', 'opso_04_model_acanthis-flammea.tar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensoundscape.torch.models.cnn import load_outdated_model\n",
    "from opensoundscape.torch.architectures import cnn_architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the load_outdated_model function will expect us to specify the model class (we'll use PytorchModel) and architecture constructor (we'll use cnn_architectures.resnet18). In this case, we also want to specify that the model should be single-tartet (models are multi-target by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created PytorchModel model object with 2 classes\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model = load_outdated_model('./opso_04_model_acanthis-flammea.tar',model_class = PytorchModel,architecture_constructor=cnn_architectures.resnet18)\n",
    "model.single_target = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now fully compatible with OpenSoundscape, and can be used as above. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acanthis-flammea-absent</th>\n",
       "      <th>acanthis-flammea-present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">./1min_audio.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>5.0</th>\n",
       "      <td>5.777370</td>\n",
       "      <td>-5.517635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <th>10.0</th>\n",
       "      <td>4.891729</td>\n",
       "      <td>-4.771999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>5.632078</td>\n",
       "      <td>-5.395758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <th>20.0</th>\n",
       "      <td>4.748441</td>\n",
       "      <td>-5.166638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <th>25.0</th>\n",
       "      <td>4.424041</td>\n",
       "      <td>-5.115609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      acanthis-flammea-absent  \\\n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                      5.777370   \n",
       "                 5.0        10.0                     4.891729   \n",
       "                 10.0       15.0                     5.632078   \n",
       "                 15.0       20.0                     4.748441   \n",
       "                 20.0       25.0                     4.424041   \n",
       "\n",
       "                                      acanthis-flammea-present  \n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                      -5.517635  \n",
       "                 5.0        10.0                     -4.771999  \n",
       "                 10.0       15.0                     -5.395758  \n",
       "                 15.0       20.0                     -5.166638  \n",
       "                 20.0       25.0                     -5.115609  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, _, _ = model.predict(prediction_dataset)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above returns the raw predictions of the model without any post-processing (such as a softmax layer or a sigmoid layer). \n",
    "\n",
    "For details on how to use the `predict()` function for post-processing of predictions and to generate binary 0/1 predictions of class presence, see the \"Basic training and prediction with CNNs\" tutorial notebook. But, as a quick example, let's add a softmax layer to make the prediction scores for both classes sum to 1. We can also use the `binary_preds` argument to generate 0/1 predictions for each sample and class. For presence/absence models, use the option `binary_preds='single_target'`. For multi-class models, think about whether each clip should be labeled with only one class (single target) or whether each clip could contain multiple classes (`binary_preds='multi_target'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2)\n"
     ]
    }
   ],
   "source": [
    "scores, binary_predictions, _ = model.predict(\n",
    "    prediction_dataset,\n",
    "    activation_layer='softmax',\n",
    "    binary_preds='single_target'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the `scores` are continuous variables, but now have been softmaxed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acanthis-flammea-absent</th>\n",
       "      <th>acanthis-flammea-present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">./1min_audio.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>5.0</th>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <th>10.0</th>\n",
       "      <td>0.999936</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>0.999984</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <th>20.0</th>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <th>25.0</th>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      acanthis-flammea-absent  \\\n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                      0.999988   \n",
       "                 5.0        10.0                     0.999936   \n",
       "                 10.0       15.0                     0.999984   \n",
       "                 15.0       20.0                     0.999951   \n",
       "                 20.0       25.0                     0.999928   \n",
       "\n",
       "                                      acanthis-flammea-present  \n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                       0.000012  \n",
       "                 5.0        10.0                      0.000064  \n",
       "                 10.0       15.0                      0.000016  \n",
       "                 15.0       20.0                      0.000049  \n",
       "                 20.0       25.0                      0.000072  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have an additional output, the binary 0/1 (\"absent\" vs \"present\") predictions generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acanthis-flammea-absent</th>\n",
       "      <th>acanthis-flammea-present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">./1min_audio.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>5.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <th>10.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <th>20.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <th>25.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      acanthis-flammea-absent  \\\n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                           1.0   \n",
       "                 5.0        10.0                          1.0   \n",
       "                 10.0       15.0                          1.0   \n",
       "                 15.0       20.0                          1.0   \n",
       "                 20.0       25.0                          1.0   \n",
       "\n",
       "                                      acanthis-flammea-present  \n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                            0.0  \n",
       "                 5.0        10.0                           0.0  \n",
       "                 10.0       15.0                           0.0  \n",
       "                 15.0       20.0                           0.0  \n",
       "                 20.0       25.0                           0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes helpful to look at a histogram of the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 316,
       "width": 874
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(scores['acanthis-flammea-present'],bins=20)\n",
    "_ = plt.xlabel('softmax score for positive class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated: Using LongAudioPreprocessor to predict on (un-split) audio files\n",
    "\n",
    "It's also possible to run predictions on long audio files by loading entire files and letting OpenSoundscape split them while predicting. This is deprecated in favor of the approach shown above and has high memory (RAM) requirements. In this case, OpenSoundscape will internally split the audio into short segments during prediction. The input dataframe in this case is simply a dataframe with file paths. The model.split_and_predict() method expects the user to provide the audio clip length.  \n",
    "\n",
    "Let's look at an example. We'll use the 1 minute audio file contained within OpenSoundscape's test folder as a \"long\" audio file. In practice, you can split files that are multiple hours long - the limiting factor is your computer's memory (\"RAM\"), which must be able to hold the entire audio file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/4c_p48ns0kl82h25nlnq3b040000gn/T/ipykernel_41480/2219618409.py:11: DeprecationWarning: Call to deprecated class LongAudioPreprocessor. (Use ClipLoadingSpectrogramPreprocessorfor similar functionality with lower memory requirements.) -- Deprecated since version 0.6.1.\n",
      "  long_audio_prediction_ds = LongAudioPreprocessor(\n"
     ]
    }
   ],
   "source": [
    "import opensoundscape\n",
    "from opensoundscape.preprocess.preprocessors import LongAudioPreprocessor\n",
    "\n",
    "#get audio path from opensoundscape's tests folder\n",
    "long_audio_prediction_df = pd.DataFrame(index=audio_files)\n",
    "img_shape = [224,224]\n",
    "\n",
    "#the audio will be split during prediction. choose the clip length and overlap of sequential clips (0 for no overlap)\n",
    "clip_length = 5.0\n",
    "clip_overlap = 0.0\n",
    "long_audio_prediction_ds = LongAudioPreprocessor(\n",
    "    long_audio_prediction_df,\n",
    "    audio_length=clip_length, \n",
    "    clip_overlap=clip_overlap, \n",
    "    out_shape=img_shape, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in addition to the scores (and potentially, predictions) the function returns a list of \"unsafe\" samples that caused errors during preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/4c_p48ns0kl82h25nlnq3b040000gn/T/ipykernel_41480/2907082553.py:1: DeprecationWarning: Call to deprecated method split_and_predict. (Use ClipLoadingSpectrogramPreprocessorwith model.predict() for similar functionality but lower memory requirements.) -- Deprecated since version 0.6.1.\n",
      "  score_df, pred_df, unsafe_samples = model.split_and_predict(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>acanthis-flammea-absent</th>\n",
       "      <th>acanthis-flammea-present</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">./1min_audio.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>5.0</th>\n",
       "      <td>5.777370</td>\n",
       "      <td>-5.517636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <th>10.0</th>\n",
       "      <td>4.891727</td>\n",
       "      <td>-4.771998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <th>15.0</th>\n",
       "      <td>5.632079</td>\n",
       "      <td>-5.395758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <th>20.0</th>\n",
       "      <td>4.748441</td>\n",
       "      <td>-5.166637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <th>25.0</th>\n",
       "      <td>4.424041</td>\n",
       "      <td>-5.115609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      acanthis-flammea-absent  \\\n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                      5.777370   \n",
       "                 5.0        10.0                     4.891727   \n",
       "                 10.0       15.0                     5.632079   \n",
       "                 15.0       20.0                     4.748441   \n",
       "                 20.0       25.0                     4.424041   \n",
       "\n",
       "                                      acanthis-flammea-present  \n",
       "file             start_time end_time                            \n",
       "./1min_audio.wav 0.0        5.0                      -5.517636  \n",
       "                 5.0        10.0                     -4.771998  \n",
       "                 10.0       15.0                     -5.395758  \n",
       "                 15.0       20.0                     -5.166637  \n",
       "                 20.0       25.0                     -5.115609  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df, pred_df, unsafe_samples = model.split_and_predict(\n",
    "    long_audio_prediction_ds,\n",
    "    file_batch_size=1,\n",
    "    num_workers=0,\n",
    "    activation_layer=None,\n",
    "    binary_preds='single_target',\n",
    "    threshold=0.5,\n",
    "    clip_batch_size=4,\n",
    "    error_log=None,\n",
    ")\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up: delete model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "for p in Path('.').glob('*.model'):\n",
    "    p.unlink()\n",
    "for p in Path('.').glob('*.tar'):\n",
    "    p.unlink()\n",
    "Path('1min_audio.wav').unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso_poetry_py38",
   "language": "python",
   "name": "opso_poetry_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
